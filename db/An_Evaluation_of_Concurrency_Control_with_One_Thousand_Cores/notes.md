论文的核心就是测试在多核的机器上测试不同的并发控制的方法的拓展性

主要使用了7种并发控制方法。

其中三种是基于锁的，分别是`Dl_DETECT`死锁检测，`NO_WAIT`获得锁失败时就abort，`WAIT_DIE`利用时间戳进行死锁预防。

四种基于时间戳，分别是`TIMESTAMP`通过记录在tuple上读写的时间戳实现串行化，`MVCC`创建tuple的多个版本，`OCC`完成工作后进行验证并判断是否有并发事务与当前事务冲突，`H-STORE`基于分区进行封锁，并根据时间戳分配锁

目前我们还是没有这么多核的计算机的，所以这个实验是在一个平台上模拟的多核CPU

将CPU组织成一个2D的网格结构，CPU共享L2缓存，并有独立的L1缓存。CPU之间通过network switch通信（模拟片上网络）

非一致存储模型，访存延迟会随着距离的增大而增大

实现中的优化：

malloc效率很低，所以实现了一种会根据工作负载自动调整内存池大小的malloc

tuple level的锁表

避免在关键路径上使用mutex，因为mutex会导致芯片之间的昂贵的消息传输。对于2PL来说，保护死锁探测器的mutex是一个瓶颈。对于T/O来说，我们也会在申请时间戳的时候用到互斥锁

优化2PL：

中心化的等待图和死锁检测算法会导致瓶颈。根据CPU核对我们用到的数据结构分区（等待图），同时使用lock-free的死锁检测

当并发度提高时，瓶颈就会出现在锁的等待上。当一个事务直到提交时才释放他获得的锁，他就会阻塞住大量其他的事务，导致出现瓶颈。

我们可以主动abort掉一些事务来减缓锁的争用。当等待锁的时间超过一个阈值的时候，我们就abort掉这个transaction

等待时间越短，abort rate越高。可以减少锁争用的问题。等待时间长则不容易abort。但是会增加锁争用的问题。所以实际中阈值的设定应取决与工作负载

优化T/O：

时间戳获取是一个比较关键的瓶颈。上面提到过使用mutex获得时间戳会导致瓶颈

一个改进的方法是使用原子加，原子加在多核的情况下效率会降低，因为这会导致维护缓存一致性上的拥塞。验证缓存副本以及写回缓存会消耗大量的总线带宽

还有一个改进的方法是使用批量的原子加，timestamp manager可以一次返回一批的time stamp来应对并发的request

还可以通过使用CPU时间加上线程ID的方法来构建独一无二的时间戳。这种方法具有较大的拓展性。在分布式数据库中，我们使用软件方法来获得时钟的同步。但是在many-core system中，使用软件方法会导致大量的开销，所以我们需要一些硬件的实现

最后还可以使用内建的硬件计数器来获得时间戳，目前没有CPU可以达成，但是在实验中他们在模拟器上实现了这一点

OCC中的验证阶段，利用per-tuple的验证来将一个验证分成若干个小的操作。于是我们可以将关键路径拆分，从而缓解mutex导致的验证阶段的瓶颈

逻辑分区，允许事务直接访问远程分区的tuple，而非使用IPC来让远程分区的worker执行操作。（其实这块我不太明白，因为对H-STORE不太熟悉）

然后是具体的实验分析，这里就不一一列了，最后列一下总结

DL_DETECT在低冲突的情况下可以拓展，但是会受到锁争用的限制

NO_WAIT拓展性较高，但是abort rate也较高。实际中的abort造成的rollback可能会导致较高的负担

WAIT_DIE受到timestamp和锁争用的限制

TIMESTAMP，会因为本地复制数据导致较高的开销。timestamp限制

MVCC表现较好，不会阻塞读或者写。timestamp限制

OCC，复制数据导致较高开销，abort代价较高。timestamp限制

H-STORE，分区工作负载下效果好。受到跨多分区的事务以及timestamp的限制

基本上所有的并发控制协议都会受到维护lock和latch的限制，MVCC虽然不会使用lock，但是每次都会生成新的record，增加内存的占用量

最后的conclusion

low core的时候，2PL的算法可以较好的处理低冲突情况下的短事务

T/O的算法可以更好的处理高冲突情况下的长事务